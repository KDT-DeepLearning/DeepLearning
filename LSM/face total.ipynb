{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sml/anaconda3/envs/Torch_PY38/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/sml/anaconda3/envs/Torch_PY38/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <ABE0EE74-6D97-3B8C-B690-C44754774FBC> /Users/sml/anaconda3/envs/Torch_PY38/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <6A598D74-186E-3808-8921-63BA99511723> /Users/sml/anaconda3/envs/Torch_PY38/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "from torch.optim.lr_scheduler import StepLR,ReduceLROnPlateau\n",
    "import torchmetrics.functional as metrics\n",
    "import os\n",
    "import shutil\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 검증용 데이터\n",
    "folder_path = '../train/train' # \n",
    "target_data = []\n",
    "img_data = []\n",
    "for encoding_label,label in enumerate(os.listdir(folder_path)):\n",
    "    label_path = os.path.join(folder_path, label)\n",
    "    if os.path.isdir(label_path):  # 디렉토리인 경우에만 진입\n",
    "        for img in os.listdir(folder_path+'/'+label):\n",
    "            image_path = os.path.join(folder_path,label,img)\n",
    "            if os.path.isfile(image_path):  # 파일인 경우에만 진입\n",
    "                with open(image_path, 'rb') as file:\n",
    "                    image = Image.open(file)\n",
    "                    width, height = image.size\n",
    "                    if width == 48 and height == 48:\n",
    "                        image_array = np.array(image)\n",
    "                        target_data.append(encoding_label)\n",
    "                        img_data.append(image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트용 데이터\n",
    "folder_path = '../test/test'\n",
    "target_test = []\n",
    "img_test = []\n",
    "for encoding_label,label in enumerate(os.listdir(folder_path)):\n",
    "    label_path = os.path.join(folder_path, label)\n",
    "    if os.path.isdir(label_path):  # 디렉토리인 경우에만 진입\n",
    "        for img in os.listdir(folder_path+'/'+label):\n",
    "            image_path = os.path.join(folder_path,label,img)\n",
    "            if os.path.isfile(image_path):  # 파일인 경우에만 진입\n",
    "                with open(image_path, 'rb') as file:\n",
    "                    image = Image.open(file)\n",
    "                    width, height = image.size\n",
    "                    if width == 48 and height == 48:\n",
    "                        image_array = np.array(image)\n",
    "                        target_test.append(encoding_label)\n",
    "                        img_test.append(image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28709, 2304)\n"
     ]
    }
   ],
   "source": [
    "# 이미지 데이터 정규화\n",
    "x_data = np.array(img_data)/255.\n",
    "x_data = x_data.reshape((-1,48*48))\n",
    "print(x_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7178, 2304)\n"
     ]
    }
   ],
   "source": [
    "x_data_test = np.array(img_test)/255.\n",
    "x_data_test = x_data_test.reshape((-1,48*48))\n",
    "print(x_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    7215\n",
       "4    4965\n",
       "5    4830\n",
       "2    4097\n",
       "0    3995\n",
       "6    3171\n",
       "1     436\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data = pd.Series(target_data).replace({0:3, 5:4, 2:5, 3:2, 6:0, 4:6, 7:1})\n",
    "target_data.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1774\n",
       "1    1247\n",
       "6    1233\n",
       "5    1024\n",
       "4     958\n",
       "2     831\n",
       "0     111\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test = pd.Series(target_test).replace({0:3, 5:4, 2:5, 3:2, 6:0, 4:6, 7:1})\n",
    "target_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 클래스 생성\n",
    "class DLdataset(Dataset):\n",
    "    \n",
    "    def __init__(self,x_data,y_data):\n",
    "        super().__init__()\n",
    "        self.feature = torch.FloatTensor(x_data)\n",
    "        self.target = torch.LongTensor(y_data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.target.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.feature[idx], self.target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "dataset = DLdataset(x_data,target_data)\n",
    "dataset_test = DLdataset(x_data_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용, 검증용 데이터 준비\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "trainDS, validDS = random_split(dataset, [0.8,0.2], generator=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치사이즈 32\n",
    "BATCH = 32\n",
    "trainDL = DataLoader(trainDS, batch_size=BATCH)\n",
    "validDL = DataLoader(validDS, batch_size=BATCH)\n",
    "testDL = DataLoader(dataset_test, batch_size=BATCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 클래스 정의\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, IN, OUT):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(IN, 128) \n",
    "        self.af = nn.ReLU()\n",
    "        self.hidden = nn.Linear(128, 32)\n",
    "        self.output = nn.Linear(32, OUT)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.input(x)\n",
    "        y = self.af(y)\n",
    "        y = self.hidden(y)\n",
    "        y = self.af(y)\n",
    "        y = self.output(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 준비\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "IN = dataset.feature.shape[1]\n",
    "OUT = pd.Series(target_data).nunique()\n",
    "\n",
    "# 모델 생성\n",
    "model  = Model(IN,OUT)\n",
    "\n",
    "# 손실함수\n",
    "LF = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# 옵티마이저\n",
    "OPTIMIZER = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# 스케줄러\n",
    "SCHEDULER = ReduceLROnPlateau(OPTIMIZER, mode = 'min', patience = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(dataLoader):\n",
    "    \n",
    "    model.train()\n",
    "    train_report=[[], []]\n",
    "    for (feature, target) in dataLoader:\n",
    "\n",
    "        feature, target = feature.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        # 학습\n",
    "        pre_target = model(feature)\n",
    "        \n",
    "        # 손실계산\n",
    "        loss = LF(pre_target, target)\n",
    "        train_report[0].append(loss)\n",
    "  \n",
    "        # 성능 평가\n",
    "        acc = metrics.accuracy(pre_target.argmax(dim=1), target, task = 'multiclass',num_classes=OUT)\n",
    "        train_report[1].append(acc)\n",
    "        \n",
    "        # W,b업데이트\n",
    "        OPTIMIZER.zero_grad()\n",
    "        loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "\n",
    "    loss_score = sum(train_report[0])/len(train_report[0])\n",
    "    acc_score = sum(train_report[1])/len(train_report[1])\n",
    "    print(f'[Train loss] ==> {loss_score}    [Train Accuracy] ==> {acc_score}')\n",
    "    return loss_score, acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(dataLoader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_report=[[], []]\n",
    "        for (feature, target)  in dataLoader:\n",
    "            # 배치크기만큼의 학습 데이터 준비\n",
    "            feature, target = feature.to(DEVICE), target.to(DEVICE)\n",
    "            \n",
    "            # 학습\n",
    "            pre_target = model(feature)\n",
    "            \n",
    "            # 손실계산\n",
    "            loss = LF(pre_target, target)\n",
    "            test_report[0].append(loss)\n",
    "      \n",
    "            # 성능 평가\n",
    "            acc = metrics.accuracy(pre_target.argmax(dim=1), target, task = 'multiclass',num_classes=OUT)\n",
    "            test_report[1].append(acc)\n",
    "    \n",
    "    loss_score = sum(test_report[0])/len(test_report[0])\n",
    "    acc_score = sum(test_report[1])/len(test_report[1])\n",
    "\n",
    "    # print(f'[Test loss] ==> {loss_score}    [Test Accuracy] ==> {acc_score}')\n",
    "    return loss_score, acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicting(dataset,n):\n",
    "\n",
    "    model = Model(IN, OUT)  # 같은 모델 구조의 객체 생성\n",
    "    model_path = \"my_trained_model.pth\"\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for feature, target in dataset:\n",
    "        # img, ytrue = dataLoader[idx][0], dataLoader[idx][1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ypre = model(img.unsqueeze(0))\n",
    "            ypre = torch.argmax(ypre, dim=1).item()\n",
    "\n",
    "            if img.shape[0] == 2304:\n",
    "                img = img.reshape(48, 48)\n",
    "\n",
    "            if ypre == ytrue:\n",
    "                correct += 1\n",
    "\n",
    "            total += 1\n",
    "            # if idx in[1, 1000, 2000, 3000, 4000, 5000, 6000, 7000] :\n",
    "            plt.imshow(img.numpy().reshape(48, 48), cmap='gray_r')\n",
    "            plt.title(f'[{idx+1}] True {ytrue} / Predict {ypre}')\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}, Correct : {correct}, Total : {total}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7178"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predicting(dataLoader,n,Model,filename):\n",
    "# # 0:angry / 1:disgust / 2:fear / 3:happy / 4:neutral / 5:sad / 6:surprise\n",
    "#     label = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "#     model_path = 'model/'+filename\n",
    "#     Model.load_state_dict(torch.load(model_path))\n",
    "#     Model.eval()\n",
    "\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     for idx in range(len(dataset)):\n",
    "#         img, ytrue = dataset[idx][0], dataset[idx][1]\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             ypre = Model(img.unsqueeze(0))\n",
    "#             ypre = torch.argmax(ypre, dim=1).item()\n",
    "\n",
    "#             if img.shape[0] == 2304:\n",
    "#                 img = img.reshape(48, 48)\n",
    "\n",
    "#             if ypre == ytrue:\n",
    "#                 correct += 1\n",
    "            \n",
    "#             total += 1\n",
    "#             if idx < n :\n",
    "#                 plt.imshow(dataset[idx][0].numpy().reshape(48, 48), cmap='gray_r')\n",
    "#                 plt.title(f'[{idx+1}] True {label[ytrue]} / Predict {label[ypre]}')\n",
    "#                 plt.xticks([])\n",
    "#                 plt.yticks([])\n",
    "#                 plt.show()\n",
    "\n",
    "\n",
    "#     accuracy = correct / total\n",
    "#     print(f'Accuracy: {accuracy:.2f}, Correct : {correct}, Total : {total}')\n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_loss = 100.0  # 초기 최소 손실 설정\n",
    "# cnt = 0\n",
    "# for eps in range(EPOCHS):\n",
    "#     print(f'[{eps+1}/{EPOCHS}]')\n",
    "#     # 학습\n",
    "#     train_loss, train_acc = training(trainDL)\n",
    "\n",
    "#     # 검증\n",
    "#     val_loss, val_acc = testing(validDL)\n",
    "    \n",
    "#     # 최소 손실 업데이트\n",
    "#     if val_loss < min_loss:\n",
    "#         min_loss = val_loss\n",
    "#         cnt = 0\n",
    "#         torch.save(model.state_dict(), \"my_trained_model.pth\")\n",
    "\n",
    "#     else:\n",
    "#         cnt+=1\n",
    "\n",
    "#     # 조기 종료 기능 => 조건 : val_loss가 지정된 횟수 이상 개선이 안되면 학습 종료\n",
    "#     if SCHEDULER.num_bad_epochs >= SCHEDULER.patience or cnt >= 5:\n",
    "#         print(f\"Early stopping at epoch {eps}\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SVC' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtraining\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainDL\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[36], line 34\u001B[0m, in \u001B[0;36mtraining\u001B[0;34m(dataLoader)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtraining\u001B[39m(dataLoader):\n\u001B[0;32m---> 34\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m()\n\u001B[1;32m     35\u001B[0m     train_report\u001B[38;5;241m=\u001B[39m[[], []]\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (feature, target) \u001B[38;5;129;01min\u001B[39;00m dataLoader:\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'SVC' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "training(trainDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SVC' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtesting\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalidDL\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[15], line 3\u001B[0m, in \u001B[0;36mtesting\u001B[0;34m(dataLoader)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtesting\u001B[39m(dataLoader):\n\u001B[0;32m----> 3\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval\u001B[49m()\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m      6\u001B[0m         test_report\u001B[38;5;241m=\u001B[39m[[], []]\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'SVC' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "testing(validDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[61], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtestDL\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "testDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[67], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mpredicting\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtestDL\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[66], line 12\u001B[0m, in \u001B[0;36mpredicting\u001B[0;34m(dataLoader, n)\u001B[0m\n\u001B[1;32m      9\u001B[0m total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(dataLoader)):\n\u001B[0;32m---> 12\u001B[0m     img, ytrue \u001B[38;5;241m=\u001B[39m \u001B[43mdataLoader\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m[\u001B[38;5;241m0\u001B[39m], dataLoader[idx][\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     15\u001B[0m         ypre \u001B[38;5;241m=\u001B[39m model(img\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m))\n",
      "\u001B[0;31mTypeError\u001B[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "predicting(testDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training with GD optimizer ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[53], line 77\u001B[0m\n\u001B[1;32m     73\u001B[0m cnt \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m eps \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(EPOCHS):\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;66;03m# print(f'[{eps+1}/{EPOCHS}]')\u001B[39;00m\n\u001B[1;32m     76\u001B[0m     \u001B[38;5;66;03m# 학습\u001B[39;00m\n\u001B[0;32m---> 77\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtraining\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainDL\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;66;03m# 검증\u001B[39;00m\n\u001B[1;32m     80\u001B[0m     val_loss, val_acc \u001B[38;5;241m=\u001B[39m testing(validDL)\n",
      "Cell \u001B[0;32mIn[53], line 53\u001B[0m, in \u001B[0;36mtraining\u001B[0;34m(dataLoader)\u001B[0m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;66;03m# W,b업데이트\u001B[39;00m\n\u001B[1;32m     52\u001B[0m     OPTIMIZER\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 53\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m     OPTIMIZER\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     56\u001B[0m loss_score \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(train_report[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(train_report[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[0;32m~/anaconda3/envs/Torch_PY38/lib/python3.8/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/Torch_PY38/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "IN = dataset.feature.shape[1]\n",
    "OUT = pd.Series(target_data).nunique()\n",
    "\n",
    "# 모델 생성\n",
    "model  = Model(IN,OUT)\n",
    "\n",
    "# 손실함수\n",
    "LF = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "OPTIMIZERLIST = [\n",
    "    (\"GD\", optim.SGD),      # Stochastic Gradient Descent\n",
    "    # (\"Momentum\", optim.SGD),       \n",
    "    # (\"NAG\", optim.SGD, {'momentum': 0.9, 'nesterov': True}), \n",
    "    (\"Adagrad\", optim.Adagrad),\n",
    "    (\"RMSProp\", optim.RMSprop),\n",
    "    # (\"Nadam\", optim.Nadam),  \n",
    "    (\"AdaDelta\", optim.Adadelta),\n",
    "    (\"Adam\", optim.Adam)\n",
    "]\n",
    "\n",
    "GDLIST = [[],[],[],[]] \n",
    "ADAGRADLIST = [[],[],[],[]]\n",
    "RMSPROPLIST = [[],[],[],[]]\n",
    "ADADELTALIST = [[],[],[],[]]\n",
    "ADAMLIST = [[],[],[],[]]\n",
    "\n",
    "\n",
    "def training(dataLoader):\n",
    "    \n",
    "    model.train()\n",
    "    train_report=[[], []]\n",
    "    for (feature, target) in dataLoader:\n",
    "\n",
    "        feature, target = feature.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        # 학습\n",
    "        pre_target = model(feature)\n",
    "        \n",
    "        # 손실계산\n",
    "        loss = LF(pre_target, target)\n",
    "        train_report[0].append(loss)\n",
    "\n",
    "        # 성능 평가\n",
    "        acc = metrics.accuracy(pre_target.argmax(dim=1), target, task = 'multiclass',num_classes=OUT)\n",
    "        train_report[1].append(acc)\n",
    "        \n",
    "        # W,b업데이트\n",
    "        OPTIMIZER.zero_grad()\n",
    "        loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "\n",
    "    loss_score = sum(train_report[0])/len(train_report[0])\n",
    "    acc_score = sum(train_report[1])/len(train_report[1])\n",
    "    # print(f'[Train loss] ==> {loss_score}    [Train Accuracy] ==> {acc_score}')\n",
    "    return loss_score, acc_score\n",
    "try:\n",
    "    for name, OPTI in OPTIMIZERLIST:\n",
    "        print(f\"\\n--- Training with {name} optimizer ---\")\n",
    "\n",
    "\n",
    "\n",
    "        # 옵티마이저\n",
    "        OPTIMIZER = OPTI(model.parameters())\n",
    "\n",
    "        # 스케줄러\n",
    "        SCHEDULER = ReduceLROnPlateau(OPTIMIZER, mode = 'min', patience = 3)\n",
    "\n",
    "        min_loss = 100.0  # 초기 최소 손실 설정\n",
    "        cnt = 0\n",
    "        for eps in range(EPOCHS):\n",
    "            # print(f'[{eps+1}/{EPOCHS}]')\n",
    "            # 학습\n",
    "            train_loss, train_acc = training(trainDL)\n",
    "\n",
    "            # 검증\n",
    "            val_loss, val_acc = testing(validDL)\n",
    "            \n",
    "            # 최소 손실 업데이트\n",
    "            if val_loss < min_loss:\n",
    "                min_loss = val_loss\n",
    "                cnt = 0\n",
    "                torch.save(model.state_dict(), name+' '+\"my_trained_model.pth\")\n",
    "\n",
    "            else:\n",
    "                cnt+=1\n",
    "\n",
    "\n",
    "            if name == 'GD':\n",
    "                GDLIST[0].append(train_loss)\n",
    "                GDLIST[1].append(train_acc)\n",
    "                GDLIST[2].append(val_loss)\n",
    "                GDLIST[3].append(val_acc)\n",
    "            elif name == 'Adagrad':\n",
    "                ADAGRADLIST[0].append(train_loss)\n",
    "                ADAGRADLIST[1].append(train_acc)\n",
    "                ADAGRADLIST[2].append(val_loss)\n",
    "                ADAGRADLIST[3].append(val_acc)\n",
    "            elif name == 'RMSProp':\n",
    "                RMSPROPLIST[0].append(train_loss)\n",
    "                RMSPROPLIST[1].append(train_acc)\n",
    "                RMSPROPLIST[2].append(val_loss)\n",
    "                RMSPROPLIST[3].append(val_acc)\n",
    "            elif name == 'AdaDelta':\n",
    "                ADADELTALIST[0].append(train_loss)\n",
    "                ADADELTALIST[1].append(train_acc)\n",
    "                ADADELTALIST[2].append(val_loss)\n",
    "                ADADELTALIST[3].append(val_acc)\n",
    "            elif name == 'Adam':\n",
    "                ADAMLIST[0].append(train_loss)\n",
    "                ADAMLIST[1].append(train_acc)\n",
    "                ADAMLIST[2].append(val_loss)\n",
    "                ADAMLIST[3].append(val_acc)\n",
    "\n",
    "\n",
    "            # 조기 종료 기능 => 조건 : val_loss가 지정된 횟수 이상 개선이 안되면 학습 종료\n",
    "            if SCHEDULER.num_bad_epochs >= SCHEDULER.patience or cnt >= 5:\n",
    "                print(f'[{eps+1}/{EPOCHS}] {name} count : {cnt}')\n",
    "                print(f'Train_loss : {train_loss}, Train_accuracy : {train_acc}')\n",
    "                print(f'Val_loss : {val_loss}, Val_accuracy : {val_acc}')\n",
    "                print(f\"Early stopping at epoch {eps}\")\n",
    "                break\n",
    "        print(f'[{eps+1}/{EPOCHS}] {name} count : {cnt}')\n",
    "        print(f'Train_loss : {train_loss}, Train_accuracy : {train_acc}')\n",
    "        print(f'Val_loss : {val_loss}, Val_accuracy : {val_acc}')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1.8687, grad_fn=<DivBackward0>), tensor(1.8234, grad_fn=<DivBackward0>), tensor(1.8129, grad_fn=<DivBackward0>), tensor(1.8081, grad_fn=<DivBackward0>), tensor(1.8043, grad_fn=<DivBackward0>), tensor(1.8005, grad_fn=<DivBackward0>), tensor(1.7967, grad_fn=<DivBackward0>), tensor(1.7928, grad_fn=<DivBackward0>), tensor(1.7887, grad_fn=<DivBackward0>), tensor(1.7845, grad_fn=<DivBackward0>), tensor(1.7801, grad_fn=<DivBackward0>), tensor(1.7755, grad_fn=<DivBackward0>), tensor(1.7706, grad_fn=<DivBackward0>), tensor(1.7652, grad_fn=<DivBackward0>), tensor(1.7597, grad_fn=<DivBackward0>), tensor(1.7539, grad_fn=<DivBackward0>), tensor(1.7480, grad_fn=<DivBackward0>), tensor(1.7418, grad_fn=<DivBackward0>), tensor(1.7356, grad_fn=<DivBackward0>), tensor(1.7294, grad_fn=<DivBackward0>), tensor(1.7234, grad_fn=<DivBackward0>), tensor(1.7176, grad_fn=<DivBackward0>), tensor(1.7121, grad_fn=<DivBackward0>), tensor(1.7069, grad_fn=<DivBackward0>), tensor(1.7021, grad_fn=<DivBackward0>), tensor(1.6975, grad_fn=<DivBackward0>), tensor(1.6933, grad_fn=<DivBackward0>), tensor(1.6893, grad_fn=<DivBackward0>), tensor(1.6856, grad_fn=<DivBackward0>), tensor(1.6820, grad_fn=<DivBackward0>), tensor(1.6787, grad_fn=<DivBackward0>), tensor(1.6756, grad_fn=<DivBackward0>), tensor(1.6726, grad_fn=<DivBackward0>), tensor(1.6697, grad_fn=<DivBackward0>), tensor(1.6670, grad_fn=<DivBackward0>), tensor(1.6644, grad_fn=<DivBackward0>), tensor(1.6619, grad_fn=<DivBackward0>), tensor(1.6595, grad_fn=<DivBackward0>), tensor(1.6572, grad_fn=<DivBackward0>), tensor(1.6549, grad_fn=<DivBackward0>), tensor(1.6528, grad_fn=<DivBackward0>), tensor(1.6506, grad_fn=<DivBackward0>), tensor(1.6486, grad_fn=<DivBackward0>), tensor(1.6466, grad_fn=<DivBackward0>), tensor(1.6446, grad_fn=<DivBackward0>), tensor(1.6427, grad_fn=<DivBackward0>), tensor(1.6408, grad_fn=<DivBackward0>), tensor(1.6390, grad_fn=<DivBackward0>), tensor(1.6371, grad_fn=<DivBackward0>), tensor(1.6353, grad_fn=<DivBackward0>), tensor(1.6336, grad_fn=<DivBackward0>), tensor(1.6318, grad_fn=<DivBackward0>), tensor(1.6300, grad_fn=<DivBackward0>), tensor(1.6283, grad_fn=<DivBackward0>), tensor(1.6266, grad_fn=<DivBackward0>), tensor(1.6248, grad_fn=<DivBackward0>), tensor(1.6231, grad_fn=<DivBackward0>), tensor(1.6214, grad_fn=<DivBackward0>), tensor(1.6197, grad_fn=<DivBackward0>), tensor(1.6181, grad_fn=<DivBackward0>), tensor(1.6164, grad_fn=<DivBackward0>), tensor(1.6147, grad_fn=<DivBackward0>), tensor(1.6131, grad_fn=<DivBackward0>), tensor(1.6114, grad_fn=<DivBackward0>), tensor(1.6097, grad_fn=<DivBackward0>), tensor(1.6080, grad_fn=<DivBackward0>), tensor(1.6064, grad_fn=<DivBackward0>), tensor(1.6047, grad_fn=<DivBackward0>), tensor(1.6031, grad_fn=<DivBackward0>), tensor(1.6015, grad_fn=<DivBackward0>), tensor(1.5999, grad_fn=<DivBackward0>), tensor(1.5982, grad_fn=<DivBackward0>), tensor(1.5966, grad_fn=<DivBackward0>), tensor(1.5950, grad_fn=<DivBackward0>), tensor(1.5933, grad_fn=<DivBackward0>), tensor(1.5917, grad_fn=<DivBackward0>), tensor(1.5901, grad_fn=<DivBackward0>), tensor(1.5885, grad_fn=<DivBackward0>), tensor(1.5869, grad_fn=<DivBackward0>), tensor(1.5853, grad_fn=<DivBackward0>), tensor(1.5837, grad_fn=<DivBackward0>), tensor(1.5821, grad_fn=<DivBackward0>), tensor(1.5805, grad_fn=<DivBackward0>), tensor(1.5789, grad_fn=<DivBackward0>), tensor(1.5774, grad_fn=<DivBackward0>), tensor(1.5758, grad_fn=<DivBackward0>), tensor(1.5743, grad_fn=<DivBackward0>), tensor(1.5727, grad_fn=<DivBackward0>), tensor(1.5712, grad_fn=<DivBackward0>), tensor(1.5696, grad_fn=<DivBackward0>), tensor(1.5681, grad_fn=<DivBackward0>), tensor(1.5665, grad_fn=<DivBackward0>), tensor(1.5650, grad_fn=<DivBackward0>), tensor(1.5635, grad_fn=<DivBackward0>), tensor(1.5619, grad_fn=<DivBackward0>), tensor(1.5604, grad_fn=<DivBackward0>), tensor(1.5589, grad_fn=<DivBackward0>), tensor(1.5574, grad_fn=<DivBackward0>), tensor(1.5559, grad_fn=<DivBackward0>), tensor(1.5544, grad_fn=<DivBackward0>)]\n",
      "\n",
      "[tensor(0.2380), tensor(0.2512), tensor(0.2512), tensor(0.2513), tensor(0.2512), tensor(0.2513), tensor(0.2516), tensor(0.2517), tensor(0.2521), tensor(0.2536), tensor(0.2564), tensor(0.2603), tensor(0.2657), tensor(0.2708), tensor(0.2763), tensor(0.2837), tensor(0.2907), tensor(0.2979), tensor(0.3044), tensor(0.3097), tensor(0.3144), tensor(0.3196), tensor(0.3229), tensor(0.3266), tensor(0.3292), tensor(0.3320), tensor(0.3351), tensor(0.3382), tensor(0.3405), tensor(0.3427), tensor(0.3450), tensor(0.3480), tensor(0.3500), tensor(0.3518), tensor(0.3533), tensor(0.3557), tensor(0.3576), tensor(0.3593), tensor(0.3601), tensor(0.3613), tensor(0.3626), tensor(0.3630), tensor(0.3644), tensor(0.3654), tensor(0.3662), tensor(0.3664), tensor(0.3669), tensor(0.3668), tensor(0.3679), tensor(0.3694), tensor(0.3696), tensor(0.3705), tensor(0.3718), tensor(0.3725), tensor(0.3736), tensor(0.3742), tensor(0.3745), tensor(0.3750), tensor(0.3756), tensor(0.3761), tensor(0.3773), tensor(0.3783), tensor(0.3795), tensor(0.3803), tensor(0.3814), tensor(0.3821), tensor(0.3827), tensor(0.3832), tensor(0.3838), tensor(0.3844), tensor(0.3849), tensor(0.3850), tensor(0.3860), tensor(0.3868), tensor(0.3871), tensor(0.3883), tensor(0.3892), tensor(0.3899), tensor(0.3905), tensor(0.3909), tensor(0.3918), tensor(0.3923), tensor(0.3928), tensor(0.3937), tensor(0.3938), tensor(0.3946), tensor(0.3952), tensor(0.3953), tensor(0.3958), tensor(0.3960), tensor(0.3966), tensor(0.3975), tensor(0.3983), tensor(0.3982), tensor(0.3989), tensor(0.3996), tensor(0.4001), tensor(0.4012), tensor(0.4018), tensor(0.4023)]\n",
      "\n",
      "[tensor(1.8364), tensor(1.8189), tensor(1.8128), tensor(1.8089), tensor(1.8052), tensor(1.8014), tensor(1.7976), tensor(1.7936), tensor(1.7895), tensor(1.7853), tensor(1.7809), tensor(1.7763), tensor(1.7715), tensor(1.7662), tensor(1.7607), tensor(1.7551), tensor(1.7493), tensor(1.7433), tensor(1.7374), tensor(1.7315), tensor(1.7259), tensor(1.7205), tensor(1.7154), tensor(1.7107), tensor(1.7062), tensor(1.7020), tensor(1.6980), tensor(1.6943), tensor(1.6907), tensor(1.6874), tensor(1.6844), tensor(1.6815), tensor(1.6787), tensor(1.6762), tensor(1.6737), tensor(1.6714), tensor(1.6692), tensor(1.6670), tensor(1.6649), tensor(1.6630), tensor(1.6610), tensor(1.6593), tensor(1.6575), tensor(1.6561), tensor(1.6544), tensor(1.6528), tensor(1.6512), tensor(1.6497), tensor(1.6482), tensor(1.6468), tensor(1.6454), tensor(1.6439), tensor(1.6426), tensor(1.6413), tensor(1.6398), tensor(1.6384), tensor(1.6370), tensor(1.6358), tensor(1.6344), tensor(1.6333), tensor(1.6318), tensor(1.6306), tensor(1.6293), tensor(1.6279), tensor(1.6265), tensor(1.6252), tensor(1.6239), tensor(1.6227), tensor(1.6213), tensor(1.6202), tensor(1.6190), tensor(1.6178), tensor(1.6166), tensor(1.6154), tensor(1.6142), tensor(1.6129), tensor(1.6118), tensor(1.6107), tensor(1.6094), tensor(1.6082), tensor(1.6072), tensor(1.6059), tensor(1.6048), tensor(1.6036), tensor(1.6024), tensor(1.6014), tensor(1.6002), tensor(1.5991), tensor(1.5982), tensor(1.5971), tensor(1.5961), tensor(1.5952), tensor(1.5943), tensor(1.5934), tensor(1.5924), tensor(1.5915), tensor(1.5903), tensor(1.5894), tensor(1.5885), tensor(1.5875)]\n",
      "\n",
      "[tensor(0.2522), tensor(0.2528), tensor(0.2528), tensor(0.2528), tensor(0.2530), tensor(0.2527), tensor(0.2539), tensor(0.2541), tensor(0.2581), tensor(0.2600), tensor(0.2638), tensor(0.2673), tensor(0.2737), tensor(0.2801), tensor(0.2841), tensor(0.2910), tensor(0.2944), tensor(0.2996), tensor(0.3059), tensor(0.3144), tensor(0.3192), tensor(0.3230), tensor(0.3266), tensor(0.3301), tensor(0.3329), tensor(0.3376), tensor(0.3397), tensor(0.3403), tensor(0.3412), tensor(0.3442), tensor(0.3459), tensor(0.3480), tensor(0.3482), tensor(0.3482), tensor(0.3482), tensor(0.3487), tensor(0.3495), tensor(0.3501), tensor(0.3511), tensor(0.3518), tensor(0.3518), tensor(0.3534), tensor(0.3549), tensor(0.3554), tensor(0.3568), tensor(0.3596), tensor(0.3610), tensor(0.3615), tensor(0.3615), tensor(0.3608), tensor(0.3617), tensor(0.3608), tensor(0.3608), tensor(0.3601), tensor(0.3610), tensor(0.3603), tensor(0.3614), tensor(0.3614), tensor(0.3627), tensor(0.3622), tensor(0.3640), tensor(0.3652), tensor(0.3660), tensor(0.3671), tensor(0.3679), tensor(0.3676), tensor(0.3681), tensor(0.3678), tensor(0.3688), tensor(0.3693), tensor(0.3688), tensor(0.3697), tensor(0.3697), tensor(0.3704), tensor(0.3709), tensor(0.3714), tensor(0.3711), tensor(0.3718), tensor(0.3732), tensor(0.3733), tensor(0.3739), tensor(0.3751), tensor(0.3764), tensor(0.3772), tensor(0.3784), tensor(0.3781), tensor(0.3783), tensor(0.3779), tensor(0.3781), tensor(0.3786), tensor(0.3791), tensor(0.3797), tensor(0.3800), tensor(0.3812), tensor(0.3814), tensor(0.3810), tensor(0.3823), tensor(0.3824), tensor(0.3835), tensor(0.3830)]\n"
     ]
    }
   ],
   "source": [
    "for g in GDLIST:\n",
    "    print(g, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1.8096, grad_fn=<DivBackward0>), tensor(1.8094, grad_fn=<DivBackward0>), tensor(1.8093, grad_fn=<DivBackward0>), tensor(1.8093, grad_fn=<DivBackward0>), tensor(1.8093, grad_fn=<DivBackward0>), tensor(1.8093, grad_fn=<DivBackward0>), tensor(1.8093, grad_fn=<DivBackward0>), tensor(1.8093, grad_fn=<DivBackward0>), tensor(1.8093, grad_fn=<DivBackward0>)]\n",
      "[tensor(0.2512), tensor(0.2512), tensor(0.2512), tensor(0.2512), tensor(0.2512), tensor(0.2512), tensor(0.2512), tensor(0.2512), tensor(0.2512)]\n",
      "[tensor(1.8109), tensor(1.8107), tensor(1.8106), tensor(1.8106), tensor(1.8106), tensor(1.8107), tensor(1.8107), tensor(1.8107), tensor(1.8107)]\n",
      "[tensor(0.2528), tensor(0.2528), tensor(0.2528), tensor(0.2528), tensor(0.2528), tensor(0.2528), tensor(0.2528), tensor(0.2528), tensor(0.2528)]\n"
     ]
    }
   ],
   "source": [
    "for a in ADAMLIST:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(x_data)\n",
    "# scaled_X_train = scaler.transform(x_data)\n",
    "# scaled_x_test = scaler.transform(x_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# model = SVC(kernel='linear')\n",
    "# model.fit(scaled_X_train, target_data)\n",
    "# train_score = model.score(scaled_X_train, target_data)\n",
    "# test_score = model.score(scaled_x_data, target_test)\n",
    "\n",
    "# print(f'Train score : {train_score}, Test score : {test_score}')\n",
    "\n",
    "# y_pred = model.predict(scaled_x_test)\n",
    "# accuracy = accuracy_score(target_test, y_pred)\n",
    "\n",
    "# print(f'{accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_PY38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
